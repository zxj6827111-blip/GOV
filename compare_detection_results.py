#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¯¹æ¯”åˆ†æå®é™…æ ·æœ¬æ–‡ä»¶çš„æ£€æµ‹æƒ…å†µ
"""

import os
import sys
import asyncio
import json
from pathlib import Path

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["FOCUS_COMPARE_ONLY"] = "0"
os.environ["ENABLE_RULES"] = "ALL"

from schemas.issues import JobContext, AnalysisConfig, create_default_config
from services.engine_rule_runner import EngineRuleRunner
from services.ai_rule_runner import run_ai_rules_batch
import pdfplumber


async def analyze_real_sample_files():
    """åˆ†æçœŸå®çš„æ ·æœ¬æ–‡ä»¶"""
    print("ğŸ” å¯¹æ¯”åˆ†æçœŸå®æ ·æœ¬æ–‡ä»¶æ£€æµ‹æƒ…å†µ")
    print("=" * 60)
    
    # æ ·æœ¬æ–‡ä»¶è·¯å¾„
    sample_files = {
        "é—®é¢˜æ–‡ä»¶1": "samples/bad/ä¸­å…±ä¸Šæµ·å¸‚æ™®é™€åŒºå§”ç¤¾ä¼šå·¥ä½œéƒ¨ 2024 å¹´åº¦éƒ¨é—¨å†³ç®—.pdf",
        "é—®é¢˜æ–‡ä»¶2": "samples/bad/ä¸Šæµ·å¸‚æ™®é™€åŒºè§„åˆ’å’Œè‡ªç„¶èµ„æºå±€ 2024 å¹´åº¦éƒ¨é—¨å†³ç®—.pdf",
        "æ­£å¸¸æ–‡ä»¶": "samples/good/ä¸Šæµ·å¸‚æ™®é™€åŒºè´¢æ”¿å±€2024å¹´åº¦éƒ¨é—¨å†³ç®—.pdf",
        "æ¨¡æ¿æ–‡ä»¶": "samples/templates/é™„ä»¶2ï¼šéƒ¨é—¨å†³ç®—æ¨¡æ¿.pdf"
    }
    
    for file_type, file_path in sample_files.items():
        print(f"\nğŸ“„ åˆ†æ {file_type}: {file_path}")
        print("-" * 40)
        
        if not Path(file_path).exists():
            print(f"  âŒ æ–‡ä»¶ä¸å­˜åœ¨")
            continue
            
        try:
            # è§£æPDF
            print("  ğŸ“– è§£æPDFæ–‡ä»¶...")
            
            page_texts = []
            page_tables = []
            
            with pdfplumber.open(file_path) as pdf:
                for page in pdf.pages:
                    page_texts.append(page.extract_text() or "")
                    
                    # æå–è¡¨æ ¼
                    tables = []
                    try:
                        # å…ˆå°è¯•çº¿æ¡ç­–ç•¥
                        tables = page.extract_tables(
                            table_settings={
                                "vertical_strategy": "lines",
                                "horizontal_strategy": "lines",
                                "intersection_tolerance": 3,
                                "min_words_vertical": 1,
                                "min_words_horizontal": 1,
                            }
                        ) or []
                        
                        # å¦‚æœæ²¡æœ‰è¡¨æ ¼ï¼Œå°è¯•é»˜è®¤ç­–ç•¥
                        if not tables:
                            tables = page.extract_tables() or []
                            
                    except Exception:
                        tables = []
                    
                    # è§„èŒƒåŒ–è¡¨æ ¼æ•°æ®
                    norm_tables = []
                    for tb in tables:
                        norm_table = [[("" if c is None else str(c)).strip() for c in row] for row in (tb or [])]
                        if norm_table:
                            norm_tables.append(norm_table)
                    
                    page_tables.append(norm_tables)
            
            print(f"  âœ… PDFè§£ææˆåŠŸ")
            print(f"  ğŸ“Š é¡µæ•°: {len(page_texts)}")
            
            # åˆ›å»ºæ–‡æ¡£å¯¹è±¡
            document = type('Document', (), {})()
            document.page_texts = page_texts
            document.page_tables = page_tables
            document.path = file_path
            document.filesize = Path(file_path).stat().st_size
            
            # é…ç½®åˆ†æå‚æ•°
            config = create_default_config()
            config.ai_enabled = True
            config.rule_enabled = True
            
            # è¿è¡ŒAIæ£€æµ‹
            print("  ğŸ¤– è¿è¡ŒAIè§„åˆ™æ£€æµ‹...")
            ai_findings = await run_ai_rules_batch(document, config)
            print(f"  âœ… AIæ£€æµ‹å®Œæˆï¼Œå‘ç°é—®é¢˜: {len(ai_findings)}")
            
            # è¿è¡Œè§„åˆ™æ£€æµ‹
            print("  ğŸ“‹ è¿è¡Œæœ¬åœ°è§„åˆ™æ£€æµ‹...")
            
            # åˆ›å»ºJobContextï¼ŒåŒ…å«è§£æçš„æ–‡æœ¬å’Œè¡¨æ ¼æ•°æ®
            job_context = JobContext(
                job_id=f"test_{file_type}",
                pdf_path=file_path,
                pages=len(page_texts),
                meta={
                    "page_texts": page_texts,
                    "page_tables": page_tables,
                    "filesize": document.filesize
                }
            )
            
            # åˆ›å»ºé…ç½®
            config = create_default_config()
            config.ai_enabled = True
            config.rule_enabled = True
            
            # è¿è¡Œæ‰€æœ‰è§„åˆ™
            from engine.rules_v33 import ALL_RULES
            rules_to_run = [
                {"id": rule.code, "code": rule.code, "title": rule.desc}
                for rule in ALL_RULES
            ]
            
            runner = EngineRuleRunner()
            rule_issues = await runner.run_rules(job_context, rules_to_run, config)
            print(f"  âœ… è§„åˆ™æ£€æµ‹å®Œæˆï¼Œå‘ç°é—®é¢˜: {len(rule_issues)}")
            
            # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
            print(f"\n  ğŸ“Š è¯¦ç»†æ£€æµ‹ç»“æœ:")
            print(f"    AIæ£€æµ‹é—®é¢˜ ({len(ai_findings)}):")
            for i, finding in enumerate(ai_findings, 1):
                print(f"      {i}. {finding.title} (ä¸¥é‡ç¨‹åº¦: {finding.severity})")
                print(f"         {finding.message}")
                print(f"         é¡µé¢: {finding.page_number}")
                
            print(f"\n    è§„åˆ™æ£€æµ‹é—®é¢˜ ({len(rule_issues)}):")
            for i, issue in enumerate(rule_issues, 1):
                print(f"      {i}. {issue.title}")
                print(f"         {issue.message}")
                if hasattr(issue, 'page_number'):
                    print(f"         é¡µé¢: {issue.page_number}")
                if hasattr(issue, 'rule_code'):
                    print(f"         è§„åˆ™: {issue.rule_code}")
                    
            # åˆ†ææ£€æµ‹æ•ˆæœ
            analyze_detection_effectiveness(file_type, ai_findings, rule_issues)
            
        except Exception as e:
            print(f"  âŒ åˆ†æå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()


def analyze_detection_effectiveness(file_type, ai_findings, rule_issues):
    """åˆ†ææ£€æµ‹æ•ˆæœ"""
    print(f"\n  ğŸ“ˆ æ£€æµ‹æ•ˆæœåˆ†æ:")
    
    # AIæ£€æµ‹åˆ†æ
    if len(ai_findings) == 0:
        print("    âš ï¸  AIæœªæ£€æµ‹åˆ°ä»»ä½•é—®é¢˜")
    else:
        print(f"    âœ… AIæ£€æµ‹åˆ° {len(ai_findings)} ä¸ªé—®é¢˜")
        
        # åˆ†æAIæ£€æµ‹çš„é—®é¢˜ç±»å‹
        severity_counts = {}
        for finding in ai_findings:
            severity = finding.severity
            severity_counts[severity] = severity_counts.get(severity, 0) + 1
            
        print(f"    ğŸ“Š é—®é¢˜ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ:")
        for severity, count in severity_counts.items():
            print(f"      - {severity}: {count}")
            
    # è§„åˆ™æ£€æµ‹åˆ†æ
    if len(rule_issues) == 0:
        print("    âš ï¸  è§„åˆ™æœªæ£€æµ‹åˆ°ä»»ä½•é—®é¢˜")
    else:
        print(f"    âœ… è§„åˆ™æ£€æµ‹åˆ° {len(rule_issues)} ä¸ªé—®é¢˜")
        
        # åˆ†æè§„åˆ™æ£€æµ‹çš„é—®é¢˜ç±»å‹
        rule_codes = {}
        for issue in rule_issues:
            code = getattr(issue, 'rule_code', 'Unknown')
            rule_codes[code] = rule_codes.get(code, 0) + 1
            
        print(f"    ğŸ“Š è§¦å‘è§„åˆ™åˆ†å¸ƒ:")
        for code, count in rule_codes.items():
            print(f"      - {code}: {count}")
            
    # ç»¼åˆåˆ†æ
    if len(ai_findings) == 0 and len(rule_issues) == 0:
        print("    ğŸ” ä¸¤ç§æ£€æµ‹æ–¹å¼éƒ½æœªå‘ç°ä»»ä½•é—®é¢˜")
        print("    ğŸ’¡ å»ºè®®:")
        print("      1. æ£€æŸ¥æ ·æœ¬æ–‡ä»¶æ˜¯å¦çœŸçš„å­˜åœ¨é—®é¢˜")
        print("      2. è°ƒæ•´æ£€æµ‹å‚æ•°å’Œé˜ˆå€¼")
        print("      3. ä¼˜åŒ–è§„åˆ™é€»è¾‘")
        print("      4. æ ¡å‡†AIæ£€æµ‹æ¨¡å‹")
    elif len(ai_findings) > 0 and len(rule_issues) == 0:
        print("    ğŸ” AIæ£€æµ‹åˆ°é—®é¢˜ä½†è§„åˆ™æœªæ£€æµ‹åˆ°")
        print("    ğŸ’¡ å¯èƒ½åŸå› :")
        print("      1. è§„åˆ™é€»è¾‘è¿‡äºä¸¥æ ¼")
        print("      2. æ–‡æ¡£è§£æä¸å‡†ç¡®")
        print("      3. AIæ£€æµ‹å¯èƒ½å­˜åœ¨è¯¯æŠ¥")
    elif len(ai_findings) == 0 and len(rule_issues) > 0:
        print("    ğŸ” è§„åˆ™æ£€æµ‹åˆ°é—®é¢˜ä½†AIæœªæ£€æµ‹åˆ°")
        print("    ğŸ’¡ å¯èƒ½åŸå› :")
        print("      1. AIæ£€æµ‹é˜ˆå€¼è¿‡é«˜")
        print("      2. AIæ¨¡å‹éœ€è¦é‡æ–°è®­ç»ƒ")
    else:
        print("    âœ… ä¸¤ç§æ£€æµ‹æ–¹å¼éƒ½å‘ç°é—®é¢˜")
        print(f"    ğŸ“Š æ£€æµ‹è¦†ç›–ç‡: {max(len(ai_findings), len(rule_issues))} ä¸ªé—®é¢˜")


async def test_specific_detection_issues():
    """æµ‹è¯•å…·ä½“çš„æ£€æµ‹é—®é¢˜"""
    print("\nğŸ” æµ‹è¯•å…·ä½“æ£€æµ‹é—®é¢˜")
    print("=" * 60)
    
    # åˆ›å»ºåŒ…å«å·²çŸ¥é—®é¢˜çš„æµ‹è¯•æ•°æ®
    problematic_texts = [
        """2024å¹´åº¦éƒ¨é—¨å†³ç®—æŠ¥å‘Š
        
        æ”¶å…¥æ”¯å‡ºå†³ç®—æ€»è¡¨
        é‡‘é¢å•ä½ï¼šä¸‡å…ƒ
        
        é¡¹ç›®                 å¹´åˆé¢„ç®—æ•°    è°ƒæ•´é¢„ç®—æ•°    å†³ç®—æ•°
        ä¸€ã€ä¸€èˆ¬å…¬å…±é¢„ç®—è´¢æ”¿æ‹¨æ¬¾  8500.00     9200.00     9150.00
        äºŒã€æ”¿åºœæ€§åŸºé‡‘é¢„ç®—è´¢æ”¿æ‹¨æ¬¾   500.00      800.00      780.00
        
        æœ¬å¹´æ”¶å…¥åˆè®¡            9000.00    10000.00     9930.00
        æ”¯å‡ºæ€»è®¡               9500.00    10500.00    10430.00  # è¿™é‡Œå­˜åœ¨å‹¾ç¨½å…³ç³»é”™è¯¯
        """,
        
        """ä¸‰å…¬ç»è´¹æ”¯å‡ºè¡¨
        é‡‘é¢å•ä½ï¼šä¸‡å…ƒ
        
        é¡¹ç›®                         å†³ç®—æ•°
        å› å…¬å‡ºå›½ï¼ˆå¢ƒï¼‰è´¹ç”¨           25.80
        å…¬åŠ¡ç”¨è½¦è´­ç½®åŠè¿è¡Œç»´æŠ¤è´¹     85.60
         å…¶ä¸­ï¼šå…¬åŠ¡ç”¨è½¦è´­ç½®è´¹        35.20
         å…¬åŠ¡ç”¨è½¦è¿è¡Œç»´æŠ¤è´¹        50.40
        å…¬åŠ¡æ¥å¾…è´¹                   18.90
        
        åˆè®¡                        130.30  # å®é™…åº”ä¸º129.70ï¼Œå­˜åœ¨è®¡ç®—é”™è¯¯
        """
    ]
    
    # å¯¹åº”çš„è¡¨æ ¼æ•°æ®
    problematic_tables = [
        [   # ç¬¬ä¸€é¡µè¡¨æ ¼ - åŒ…å«å‹¾ç¨½å…³ç³»é”™è¯¯
            [["é¡¹ç›®"], ["å¹´åˆé¢„ç®—æ•°"], ["è°ƒæ•´é¢„ç®—æ•°"], ["å†³ç®—æ•°"]],
            [["ä¸€ã€ä¸€èˆ¬å…¬å…±é¢„ç®—è´¢æ”¿æ‹¨æ¬¾"], ["8500.00"], ["9200.00"], ["9150.00"]],
            [["äºŒã€æ”¿åºœæ€§åŸºé‡‘é¢„ç®—è´¢æ”¿æ‹¨æ¬¾"], ["500.00"], ["800.00"], ["780.00"]],
            [["æœ¬å¹´æ”¶å…¥åˆè®¡"], ["9000.00"], ["10000.00"], ["9930.00"]],
            [["æ”¯å‡ºæ€»è®¡"], ["9500.00"], ["10500.00"], ["10430.00"]]  # é”™è¯¯ï¼šæ”¶å…¥9930ï¼Œæ”¯å‡º10430
        ],
        [   # ç¬¬äºŒé¡µè¡¨æ ¼ - åŒ…å«è®¡ç®—é”™è¯¯
            [["é¡¹ç›®"], ["å†³ç®—æ•°"]],
            [["å› å…¬å‡ºå›½ï¼ˆå¢ƒï¼‰è´¹ç”¨"], ["25.80"]],
            [["å…¬åŠ¡ç”¨è½¦è´­ç½®åŠè¿è¡Œç»´æŠ¤è´¹"], ["85.60"]],
            [["å…¶ä¸­ï¼šå…¬åŠ¡ç”¨è½¦è´­ç½®è´¹"], ["35.20"]],
            [["å…¬åŠ¡ç”¨è½¦è¿è¡Œç»´æŠ¤è´¹"], ["50.40"]],
            [["å…¬åŠ¡æ¥å¾…è´¹"], ["18.90"]],
            [["åˆè®¡"], ["130.30"]]  # é”™è¯¯ï¼šå®é™…åº”ä¸º129.70
        ]
    ]
    
    # åˆ›å»ºæµ‹è¯•æ–‡æ¡£
    document = type('Document', (), {})()
    document.page_texts = problematic_texts
    document.page_tables = problematic_tables
    document.path = "test_problematic_report.pdf"
    document.filesize = 500000
    
    print("ğŸ“„ æµ‹è¯•åŒ…å«å·²çŸ¥é—®é¢˜çš„æ–‡æ¡£")
    print("å·²çŸ¥é—®é¢˜:")
    print("  1. æ”¶å…¥æ”¯å‡ºå‹¾ç¨½å…³ç³»é”™è¯¯ï¼ˆæ”¶å…¥9930ä¸‡ï¼Œæ”¯å‡º10430ä¸‡ï¼‰")
    print("  2. ä¸‰å…¬ç»è´¹è®¡ç®—é”™è¯¯ï¼ˆå®é™…129.70ä¸‡ï¼Œæ˜¾ç¤º130.30ä¸‡ï¼‰")
    
    # é…ç½®åˆ†æå‚æ•°
    config = create_default_config()
    config.ai_enabled = True
    config.rule_enabled = True
    
    # è¿è¡ŒAIæ£€æµ‹
    print("\nğŸ¤– è¿è¡ŒAIè§„åˆ™æ£€æµ‹...")
    ai_findings = await run_ai_rules_batch(document, config)
    print(f"âœ… AIæ£€æµ‹å®Œæˆï¼Œå‘ç°é—®é¢˜: {len(ai_findings)}")
    
    for i, finding in enumerate(ai_findings, 1):
        print(f"  {i}. {finding.title} (ä¸¥é‡ç¨‹åº¦: {finding.severity})")
        print(f"     {finding.message}")
        print(f"     é¡µé¢: {finding.page_number}")
        if finding.evidence:
            print(f"     è¯æ®: {finding.evidence}")
            
    # è¿è¡Œè§„åˆ™æ£€æµ‹
    print("\nğŸ“‹ è¿è¡Œæœ¬åœ°è§„åˆ™æ£€æµ‹...")
    
    # åˆ›å»ºJobContextï¼ŒåŒ…å«è§£æçš„æ–‡æœ¬å’Œè¡¨æ ¼æ•°æ®
    job_context = JobContext(
        job_id="test_problematic",
        pdf_path="test_problematic_report.pdf",
        pages=len(document.page_texts),
        meta={
            "page_texts": document.page_texts,
            "page_tables": document.page_tables,
            "filesize": document.filesize
        }
    )
    
    # åˆ›å»ºé…ç½®
    config = create_default_config()
    config.ai_enabled = True
    config.rule_enabled = True
    
    # è¿è¡Œæ‰€æœ‰è§„åˆ™
    from engine.rules_v33 import ALL_RULES
    rules_to_run = [
        {"id": rule.code, "code": rule.code, "title": rule.desc}
        for rule in ALL_RULES
    ]
    
    runner = EngineRuleRunner()
    rule_issues = await runner.run_rules(job_context, rules_to_run, config)
    print(f"âœ… è§„åˆ™æ£€æµ‹å®Œæˆï¼Œå‘ç°é—®é¢˜: {len(rule_issues)}")
    
    for i, issue in enumerate(rule_issues, 1):
        print(f"  {i}. {issue.title}")
        print(f"     {issue.message}")
        if hasattr(issue, 'page_number'):
            print(f"     é¡µé¢: {issue.page_number}")
        if hasattr(issue, 'rule_code'):
            print(f"     è§„åˆ™: {issue.rule_code}")


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” æ·±åº¦åˆ†ææ£€æµ‹é—®é¢˜å¯¹æ¯”")
    print("=" * 60)
    
    # 1. åˆ†æçœŸå®æ ·æœ¬æ–‡ä»¶
    await analyze_real_sample_files()
    
    # 2. æµ‹è¯•å…·ä½“æ£€æµ‹é—®é¢˜
    await test_specific_detection_issues()
    
    print("\nâœ… å¯¹æ¯”åˆ†æå®Œæˆï¼")


if __name__ == "__main__":
    asyncio.run(main())